# Mira: Model-Integrated Routing & Adaptation

Mira æ˜¯ä¸€ç§æ”¯æŒ vllmï¼Œtransformersï¼Œç¬¬ä¸‰æ–¹ api æ¥å£çš„ç»Ÿä¸€éƒ¨ç½²è°ƒç”¨åº“ï¼Œè®¾è®¡ç›®çš„æ˜¯ä¸ºäº†æ›´åŠ è‡ªç”±åœ°è®¾è®¡ agentï¼Œèƒ½å¤Ÿè·å–æ¦‚ç‡ï¼Œæ³¨æ„åŠ›ç­‰å¤æ‚ä¿¡æ¯ï¼Œç”¨æˆ·å¯ä»¥è‡ªå·±ä½¿ç”¨ç¬¬ä¸‰æ–¹æ¨¡å‹ï¼Œæˆ–è€…è‡ªå·±éƒ¨ç½²æ¨¡å‹ï¼Œç„¶åä½¿ç”¨ç»Ÿä¸€çš„æ¥å£è°ƒåº¦è®¾è®¡ agent

## ğŸŒŸ Features

- å…¼å®¹ OpenAI åè®®
- æ›´åŠ æ–¹ä¾¿åœ° Rollout è·å–æ¦‚ç‡å’Œ pass@K æ ·æœ¬
- æ”¯æŒæœ¬åœ° vLLMã€HF Transformers æ¨¡å‹
- å…¼å®¹ OpenAIã€Claudeã€Geminiã€OpenRouterã€Qwenã€Seed ç­‰æ¨¡å‹ï¼ˆç¬¬ä¸‰æ–¹çš„é€šå¸¸æ‹¿ä¸åˆ°æ¦‚ç‡å’Œæ³¨æ„åŠ›ï¼‰
- æ”¯æŒé‡‡ç”¨ BaseModel ç±»å®ç°è‡ªå®šä¹‰çš„å‡½æ•° toolï¼Œæä¾› tool è¿è¡Œçš„çº¿ç¨‹ç®¡ç†ï¼Œæ–¹ä¾¿ç”¨æˆ·è®¾è®¡é‡å‹ã€å¤æ‚ã€å¸¦çŠ¶æ€çš„å‡½æ•°åŠŸèƒ½
- æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰ä¸Šä¸‹æ–‡å·¥ç¨‹å¤„ç†ï¼Œæ”¯æŒæ›´åŠ çµæ´»çš„ agent è®¾è®¡

## TODO-LIST
- [ ] HF Transformers åŸç”Ÿæ¨¡å‹éƒ¨ç½²æ”¯æŒ
- [ ] èƒ½å¤Ÿè¾“å‡º token level entropy ä»¥åŠçŸ­åºåˆ—çš„ attention
- [ ] æ–¹ä¾¿åç»­ç”µè·¯å›¾ç”Ÿæˆï¼Œç”¨äºå¯è§£é‡Š
- [ ] å…¼å®¹ OpenAI harmony åè®®

## ğŸš€ Installation

### Prerequisites
- Ubuntu >= 22.04 or Centos >= 7
- CUDA-compatible GPU (for local inference, better for cuda 12.4+)

### Using Conda (Recommended)

```bash
# Create a new environment
conda create -n mira python=3.11
conda activate mira

# Clone the repository
git clone https://github.com/yourusername/mira.git
cd mira

# Install dependencies
pip install --upgrade pip setuptools
pip install -e .
```

## âš™ï¸ Configuration

1.  **Environment Variables**: Copy the template to create your local config.

    ```bash
    cp .env.template .env
    ```

2.  **Edit `.env`**: Fill in your API keys and preferences.

    ```ini
    # Local Inference Settings
    CUDA_VISIBLE_DEVICES=0
    
    # API Keys (Fill as needed)
    OPENAI_API_KEY=sk-...
    OPENROUTER_API_KEY=sk-...
    HF_TOKEN=hf_...
    ```

## ğŸ“– Usage

### vllm æœåŠ¡å¯åŠ¨

```python
python -m mira.oai_protocol --model Qwen/Qwen3-8B
```

### 2. OpenAI-Compatible Server

Start an API server that mimics OpenAI's interface, serving your local models or routing requests.

```bash
# Start the server (example command, adjust based on your entry point)
python -m mira.oai_protocol --model Qwen/Qwen3-8B --host 0.0.0.0 --port 8000
```

*Note: Check `mira/oai_protocol.py` for specific CLI arguments.*

### 3. Using Remote APIs

Mira can act as a client for various LLM providers.

```python
from mira.openrouter import OpenRouterClient

client = OpenRouterClient()
response = client.chat.completions.create(
    model="openai/gpt-4",
    messages=[{"role": "user", "content": "Tell me a joke."}]
)
print(response.choices[0].message.content)
```

## ğŸ“‚ Project Structure

```
mira/
â”œâ”€â”€ demo/               # Example scripts and tests
â”œâ”€â”€ mira/
â”‚   â”œâ”€â”€ inference.py    # Core inference engines (vLLM, HF)
â”‚   â”œâ”€â”€ oai_protocol.py # OpenAI API server implementation
â”‚   â”œâ”€â”€ openrouter.py   # OpenRouter and remote API clients
â”‚   â”œâ”€â”€ args.py         # Configuration and argument parsing
â”‚   â””â”€â”€ types.py        # Type definitions
â”œâ”€â”€ .env.template       # Environment variable template
â”œâ”€â”€ pyproject.toml      # Project dependencies and build config
â””â”€â”€ README.md           # This file
```

## ğŸ—‘ï¸ Uninstallation

```bash
pip uninstall mira
# If using Conda
conda env remove -n mira
```

## ğŸ“„ License

MIT License